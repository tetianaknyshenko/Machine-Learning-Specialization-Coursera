# Machine-Learning-Specialization-Coursera
–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è (Linear Regression)

–ú–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏.
–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (Regularization to Avoid Overfitting)

–¢–µ—Ö–Ω–∏–∫–∞, —É–º–µ–Ω—å—à–∞—é—â–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –µ—ë –∞–¥–∞–ø—Ç–∞—Ü–∏—é –∫ —à—É–º—É –≤ –¥–∞–Ω–Ω—ã—Ö.
–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (Logistic Regression for Classification)

–ú–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–∞/–Ω–µ—Ç, 0/1).
–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (Gradient Descent)

–ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ—à–∏–±–∫—É –º–æ–¥–µ–ª–∏, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Learning)

–ú–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
üá¨üáß Brief Explanation in English:
Linear Regression: Predicts continuous outcomes based on relationships between variables.
Regularization to Avoid Overfitting: Reduces model complexity to prevent it from fitting noise in the data.
Logistic Regression for Classification: Used for predicting categorical outcomes (e.g., yes/no).
Gradient Descent: An optimization algorithm that minimizes model error by adjusting parameters.
Supervised Learning: A machine learning method where a model learns from labeled data.
üá©üá™ Kurze Erkl√§rung auf Deutsch:
Lineare Regression: Sagt kontinuierliche Werte basierend auf Variablenbeziehungen voraus.
Regularisierung zur Vermeidung von Overfitting: Reduziert die Komplexit√§t des Modells, um √úberanpassung zu vermeiden.
Logistische Regression f√ºr Klassifikation: Wird verwendet, um kategoriale Ergebnisse vorherzusagen (z.B. ja/nein).
Gradientenabstieg: Ein Optimierungsalgorithmus, der Modellfehler durch Anpassung von Parametern minimiert.
√úberwachtes Lernen (Supervised Learning): Eine Methode des maschinellen Lernens, bei der das Modell mit beschrifteten Daten trainiert wird.





Gradient Descent grad spusk

Derivative –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è 
Convergence - —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å:
 –§–∞–∫—Ç–æ—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å:
Learning Rate (–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è) ‚Äî —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π —à–∞–≥ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π ‚Äî –∫ –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
–§–æ—Ä–º–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å (Loss Function) ‚Äî –≤—ã–ø—É–∫–ª–æ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–º–æ–≥–∞–µ—Ç –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å.
–®—É–º –≤ –¥–∞–Ω–Ω—ã—Ö ‚Äî —Å–∏–ª—å–Ω—ã–π —à—É–º –º–æ–∂–µ—Ç –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—é —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ –º–∏–Ω–∏–º—É–º–∞.


tangent line - kasatelmzfy
slope the tangent line - –Ω–∞–∫–ª–æ–Ω –∫–∞—Å–∞—Ç–µ–ª—å–Ω–æ–π
Gradient descent:
https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21
https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3#:~:text=In%20this%20article%2C%20we%E2%80%99ll%20cover%20gradient%20descent%20algorithm,before%20going%20into%20the%20details%20of%20its%20variants.
https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/

–ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —á–∏—Å–µ–ª, —Å–∏–º–≤–æ–ª–æ–≤ –∏–ª–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏–π –≤ —Å—Ç—Ä–æ–∫–∞—Ö –∏ —Å—Ç–æ–ª–±—Ü–∞—Ö. –ß—Ç–æ–±—ã —É–º–Ω–æ–∂–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—ã, –≤–∞–º –Ω—É–∂–Ω–æ —É–º–Ω–æ–∂–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã (–∏–ª–∏ —á–∏—Å–ª–∞) –≤ —Å—Ç—Ä–æ–∫–∞—Ö –ø–µ—Ä–≤–æ–π –º–∞—Ç—Ä–∏—Ü—ã –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö –≤—Ç–æ—Ä–æ–π –º–∞—Ç—Ä–∏—Ü—ã –∏ —Å–ª–æ–∂–∏—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.–°—Ç—Ä–æ–∫—É —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ —Å—Ç–æ–ª–±–∏–∫
2  -3     9  -6        2*9-3*6     2*-6-3*-4         0   0
4  -6    6   -4        4*9-6*6    4*-6-6*-4          0   0

9  -6    2  -3         9*2-6*4   9*-3-6*-6         -6  9
6  -4    4  -6         6*2-4*4   6*-3+24           -4  6
The NumPy dot function is able to use parallel hardware in your computer and this is true whether you're running this on a normal computer, that is on a normal computer CPU or if you are using a GPU, a graphics processor unit, that's often used to accelerate machine learning jobs. The ability of the NumPy dot function to use parallel hardware makes it much more efficient than the for loop or the sequential calculation that we saw previously. Now, this version is much more practical when n is large because you are not typing w0 times x0 plus w1 times x1 plus lots of additional terms like you would have had for the previous version. 
